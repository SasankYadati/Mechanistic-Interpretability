# Mechanistic-Interpretability
Reverse engineering trained models to understand how they do what they do.

## Transformer from scratch
Build and train a small GPT-like model on WikiText-2 dataset.

## Getting comfortable with tooling
There are tools like TransformerLens that should help understand the trained models. Get familiar enough with them to start working on the enxt step. 

## Working on a concrete problem
Take a small toy problem like fibonacci sequence modelling, training a transformer model on this problem and reverse engineering it to understand how it solves the problem.

## Publish
Publish what I learned from the above steps.
